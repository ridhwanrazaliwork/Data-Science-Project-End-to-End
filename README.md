# End-to-End Data Science Pipeline

A complete machine learning pipeline for predicting wine quality using the Wine Quality dataset. This project demonstrates best practices in data science with a clean folder structure, modular design, and integrated experiment tracking.

## ğŸ“‹ Project Overview

This pipeline automates the entire ML workflow from data ingestion to model evaluation. It uses **ElasticNet regression** to predict wine quality scores and tracks all experiments with **MLflow** on **DagsHub**.

### Dataset
- **Wine Quality Dataset** (red wine): 1,599 samples with 11 physicochemical features
- **Target**: Wine quality (ratings from 3â€“8)
- **Source**: Automatically downloaded from GitHub during pipeline execution

## ğŸ› ï¸ Tech Stack

- **Python 3.10+**
- **scikit-learn**: ElasticNet model, preprocessing
- **MLflow**: Experiment tracking & model registry (hosted on DagsHub)
- **Flask**: Web framework
- **HTML/CSS**: Simple web UI for predictions
- **YAML**: Configuration management

## ğŸ“ Project Structure

```
basic_project/
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.yaml          # Data paths, model params
â”‚   â”œâ”€â”€ schema.yaml          # Dataset schema validation
â”‚   â””â”€â”€ params.yaml          # Model hyperparameters
â”œâ”€â”€ src/datascience/
â”‚   â”œâ”€â”€ components/          # Modular pipeline stages
â”‚   â”‚   â”œâ”€â”€ data_ingestion.py
â”‚   â”‚   â”œâ”€â”€ data_validation.py
â”‚   â”‚   â”œâ”€â”€ data_transformation.py
â”‚   â”‚   â”œâ”€â”€ model_trainer.py
â”‚   â”‚   â””â”€â”€ model_evaluation.py
â”‚   â”œâ”€â”€ pipeline/            # Pipeline orchestrators
â”‚   â”œâ”€â”€ config/              # Configuration manager
â”‚   â”œâ”€â”€ entity/              # Data classes
â”‚   â”œâ”€â”€ utils/               # Helpers (YAML, logging)
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ artifacts/               # Generated outputs (data, models, metrics)
â”œâ”€â”€ main.py                  # Pipeline entry point
â”œâ”€â”€ requirements.txt         # Dependencies
â””â”€â”€ README.md
```

## ğŸ”„ ML Pipeline Stages

1. **Data Ingestion** â€” Downloads & extracts wine quality dataset
2. **Data Validation** â€” Validates schema consistency
3. **Data Transformation** â€” Feature preprocessing & train/test split
4. **Model Trainer** â€” Trains ElasticNet regression model
5. **Model Evaluation** â€” Computes metrics (RMSE, MAE, RÂ²) and logs to MLflow

### Pipeline Flow Diagram

```mermaid
graph TD
    Start([Start Pipeline]) --> DI["Data Ingestion Stage"]
    DI -->|Success| DV["Data Validation Stage"]
    DI -->|Error| Error["âš ï¸ Error Handling<br/>Log & Raise Exception"]
    
    DV -->|Success| DT["Data Transformation Stage"]
    DV -->|Error| Error
    
    DT -->|Success| MT["Model Trainer Stage"]
    DT -->|Error| Error
    
    MT -->|Success| ME["Model Evaluation Stage"]
    MT -->|Error| Error
    
    ME -->|Success| End["âœ… Pipeline Complete"]
    ME -->|Error| Error
    
    Error --> End
    
    style Start fill:#90EE90
    style End fill:#90EE90
    style Error fill:#FFB6C6
    style DI fill:#87CEEB
    style DV fill:#87CEEB
    style DT fill:#87CEEB
    style MT fill:#87CEEB
    style ME fill:#87CEEB
```

## ğŸš€ Quick Start

### Prerequisites
- Python 3.10+
- Conda or venv

### Installation

```bash
# Install dependencies
pip install -r requirements.txt

# Set MLflow credentials (optional, for DagsHub integration)
export MLFLOW_TRACKING_URI="your_mlflow_tracking_url"
export MLFLOW_TRACKING_USERNAME="your_dagshub_username"
export MLFLOW_TRACKING_PASSWORD="your_dagshub_token"
```

### Run Full Pipeline

```bash
# Execute all stages (ingestion â†’ evaluation)
python main.py
```

### Run Individual Stages

Edit `main.py` to comment/uncomment pipeline stages as needed.

## ğŸ“Š Experiment Tracking

All model runs are logged to **MLflow** including:
- Model hyperparameters (alpha, l1_ratio)
- Evaluation metrics (RMSE, MAE, RÂ²)
- Trained model artifacts

## ğŸŒ Web UI (Optional)

A Flask app with simple HTML interface for predictions is available in the `app/` directory.

## ğŸ“ Configuration

Update pipeline behavior by editing:
- `config/config.yaml` â€” Data paths, artifact locations
- `config/params.yaml` â€” Model hyperparameters (alpha, l1_ratio)
- `config/schema.yaml` â€” Expected dataset columns

## ğŸ“¦ Key Dependencies

- scikit-learn
- pandas
- numpy
- mlflow
- flask
- pyyaml

See `requirements.txt` for complete list.